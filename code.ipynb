{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23e299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries installed and ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets, Tokenizer, and Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/600 [00:00<?, ? examples/s]c:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:00<00:00, 6691.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\vuvan\\AppData\\Local\\Temp\\ipykernel_15660\\3170262463.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/3000 00:17 < 7:14:17, 0.11 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Start the training!\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Starting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m trainer.train()\n\u001b[32m    119\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéâ Training finished!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# 6. EVALUATION & INFERENCE: Use the trained model\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Save the final model so you can use it later\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\transformers\\trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2207\u001b[39m         args=args,\n\u001b[32m   2208\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2209\u001b[39m         trial=trial,\n\u001b[32m   2210\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2211\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\transformers\\trainer.py:2607\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2603\u001b[39m         grad_norm = _grad_norm\n\u001b[32m   2605\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2607\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m   2609\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2611\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\accelerate\\optimizer.py:179\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28mself\u001b[39m.optimizer.step(closure)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func.\u001b[34m__get__\u001b[39m(opt, opt.\u001b[34m__class__\u001b[39m)(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     adam(\n\u001b[32m    247\u001b[39m         params_with_grad,\n\u001b[32m    248\u001b[39m         grads,\n\u001b[32m    249\u001b[39m         exp_avgs,\n\u001b[32m    250\u001b[39m         exp_avg_sqs,\n\u001b[32m    251\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    252\u001b[39m         state_steps,\n\u001b[32m    253\u001b[39m         amsgrad=group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    254\u001b[39m         has_complex=has_complex,\n\u001b[32m    255\u001b[39m         beta1=beta1,\n\u001b[32m    256\u001b[39m         beta2=beta2,\n\u001b[32m    257\u001b[39m         lr=group[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    258\u001b[39m         weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    259\u001b[39m         eps=group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    260\u001b[39m         maximize=group[\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    261\u001b[39m         foreach=group[\u001b[33m\"\u001b[39m\u001b[33mforeach\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    262\u001b[39m         capturable=group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    263\u001b[39m         differentiable=group[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    264\u001b[39m         fused=group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    265\u001b[39m         grad_scale=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrad_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    266\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    267\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m func(\n\u001b[32m    934\u001b[39m     params,\n\u001b[32m    935\u001b[39m     grads,\n\u001b[32m    936\u001b[39m     exp_avgs,\n\u001b[32m    937\u001b[39m     exp_avg_sqs,\n\u001b[32m    938\u001b[39m     max_exp_avg_sqs,\n\u001b[32m    939\u001b[39m     state_steps,\n\u001b[32m    940\u001b[39m     amsgrad=amsgrad,\n\u001b[32m    941\u001b[39m     has_complex=has_complex,\n\u001b[32m    942\u001b[39m     beta1=beta1,\n\u001b[32m    943\u001b[39m     beta2=beta2,\n\u001b[32m    944\u001b[39m     lr=lr,\n\u001b[32m    945\u001b[39m     weight_decay=weight_decay,\n\u001b[32m    946\u001b[39m     eps=eps,\n\u001b[32m    947\u001b[39m     maximize=maximize,\n\u001b[32m    948\u001b[39m     capturable=capturable,\n\u001b[32m    949\u001b[39m     differentiable=differentiable,\n\u001b[32m    950\u001b[39m     grad_scale=grad_scale,\n\u001b[32m    951\u001b[39m     found_inf=found_inf,\n\u001b[32m    952\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m    953\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vuvan\\Desktop\\An_Plaza\\Temporal_QA\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:439\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    436\u001b[39m     device_beta1 = beta1\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m exp_avg.lerp_(grad, \u001b[32m1\u001b[39m - device_beta1)\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. SETUP: Install necessary libraries\n",
    "# ==============================================================================\n",
    "# !pip install transformers[torch] datasets sentencepiece --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed and ready.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA PREPARATION: Create our training and testing datasets\n",
    "# ==============================================================================\n",
    "import json\n",
    "\n",
    "# For a real project, you would have hundreds or thousands of examples.\n",
    "# We'll create two small files for this demonstration.\n",
    "# train_data = [\n",
    "#     {\"question\": \"Th·ªùi gian 10 th√°ng tr∆∞·ªõc th√°ng 2, 1130 l√† khi n√†o?\", \"answer\": \"Th√°ng 4, 1129\"},\n",
    "#     {\"question\": \"What is 3 weeks after January 1, 2024?\", \"answer\": \"January 22, 2024\"},\n",
    "#     {\"question\": \"5 ng√†y tr∆∞·ªõc ng√†y 10 th√°ng 7 nƒÉm 2025 l√† ng√†y n√†o?\", \"answer\": \"Ng√†y 5 th√°ng 7 nƒÉm 2025\"},\n",
    "#     {\"question\": \"What date is 2 months after February 29, 2024?\", \"answer\": \"April 29, 2024\"},\n",
    "#     {\"question\": \"1 nƒÉm sau ng√†y 15 th√°ng 5, 2030 l√† ng√†y g√¨?\", \"answer\": \"Ng√†y 15 th√°ng 5, 2031\"}\n",
    "# ]\n",
    "\n",
    "# test_data = [\n",
    "#     {\"question\": \"What is 4 days before March 1, 2024?\", \"answer\": \"February 26, 2024\"},\n",
    "#     {\"question\": \"2 tu·∫ßn sau ng√†y 20 th√°ng 12 nƒÉm 2025 l√† ng√†y n√†o?\", \"answer\": \"Ng√†y 3 th√°ng 1 nƒÉm 2026\"}\n",
    "# ]\n",
    "\n",
    "# # Write the data to JSONL files (one JSON object per line)\n",
    "# with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "#     for item in train_data:\n",
    "#         f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "#     for item in test_data:\n",
    "#         f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# print(\"‚úÖ train.jsonl and test.jsonl files created.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LOAD DATA, TOKENIZER, and MODEL\n",
    "# ==============================================================================\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load the datasets from our files\n",
    "raw_datasets = load_dataset('json', data_files={'train': 'date_train.jsonl', 'test': 'date_test.jsonl'})\n",
    "\n",
    "# Define the model checkpoint we want to use. 't5-small' is great for fast fine-tuning.\n",
    "model_checkpoint = \"google/mt5-base\"\n",
    "\n",
    "# Load the tokenizer to preprocess the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load the model itself\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"‚úÖ Datasets, Tokenizer, and Model loaded.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PREPROCESSING: Convert text to tokens the model can understand\n",
    "# ==============================================================================\n",
    "# T5 models expect a prefix for the task. We'll create one.\n",
    "prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenizes the questions and answers, ensuring answers are strings.\"\"\"\n",
    "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # The FIX is here: Ensure every item in the \"answer\" list is a string.\n",
    "    # This prevents errors if your JSONL file has null/None values.\n",
    "    answers = [str(ans) if ans is not None else \"\" for ans in examples[\"answer\"]]\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(answers, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing to our entire dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing complete.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAINING: Fine-tune the model on our data\n",
    "# ==============================================================================\n",
    "# Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",           # Directory to save the model\n",
    "    eval_strategy=\"epoch\",      # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,               # The speed at which the model learns\n",
    "    per_device_train_batch_size=4,    # Batch size for training\n",
    "    per_device_eval_batch_size=4,     # Batch size for evaluation\n",
    "    num_train_epochs=5,               # Number of times to go through the data\n",
    "    weight_decay=0.01,                # Regularization to prevent overfitting\n",
    "    save_total_limit=1,               # Only keep the best model\n",
    "    predict_with_generate=True,       # Needed for text generation evaluation\n",
    ")\n",
    "\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start the training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"üéâ Training finished!\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EVALUATION & INFERENCE: Use the trained model\n",
    "# ==============================================================================\n",
    "# Save the final model so you can use it later\n",
    "final_model_path = \"./final_date_calculator_model\"\n",
    "trainer.save_model(final_model_path)\n",
    "print(f\"‚úÖ Model saved to {final_model_path}\")\n",
    "\n",
    "# Load our fine-tuned model and tokenizer using a pipeline for easy inference\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"\\n\\nüß™ Running inference with the fine-tuned model...\")\n",
    "question = \"6 th√°ng sau th√°ng 9 nƒÉm 2025 l√† khi n√†o?\"\n",
    "\n",
    "# Load the trained model using the pipeline helper\n",
    "date_calculator = pipeline(\"text2text-generation\", model=final_model_path)\n",
    "result = date_calculator(prefix + question)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Predicted Answer: {result[0]['generated_text']}\")\n",
    "# Expected output for the question above should be \"Th√°ng 3 nƒÉm 2026\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a6b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDNN version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb335330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:00<00:00, 9351.22 examples/s]\n",
      "C:\\Users\\vuvan\\AppData\\Local\\Temp\\ipykernel_21612\\4096341067.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Continuing training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150000' max='150000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150000/150000 3:08:31, Epoch 250/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.046563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.046128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.045563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.045050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.044785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.044569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.044239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.043682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.043129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.042807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.042306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.041753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.041712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.041106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.040766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.040672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.040398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.040246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.039767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.039475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.038987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.038931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.038555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.038321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.038192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.037911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.037670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.037155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.037086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.037074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.036984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.036540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.036428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.035856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.035508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.035380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.035143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.035037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.034615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.034472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.034352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.034045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.033858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.033557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.033214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.032865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.032571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.032524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.031945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.031662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.031134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.030642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.030691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.029924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.029989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.029775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.029396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.029189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.028721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.028794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.028551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.028553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.028307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.027988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.027495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.027843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.027477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.027166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.027163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.026540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.026657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.026482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.026285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.026198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.026292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.025947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.025773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.025680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.025619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.025599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.025205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.025163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.025209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.025054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.025134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.025015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.024816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.024703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.024644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.024753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.024759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.024559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.024362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.024259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.024324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.024053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.024112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.023881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.023952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.023902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.024011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.023877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.024006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.023851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.023842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.023697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.023679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.023641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.023711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.023541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.023467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.023597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.023414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.023415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.023497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.023469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.023453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.023365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.023531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.023537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.023578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.023336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.023432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.023237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.023523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.023351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.023352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.023327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.023587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.023411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.023551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.023310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.023391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.023290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.023287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.023469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.023360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.023251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.023398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.023404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.023448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.023455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.023505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.023574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.023524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.023769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.023727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.023419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.023213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.023523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.023364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.023560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.023641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.023350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.023436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.023487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.023448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.023534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.023639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.023562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.023542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.023656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.023489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.023593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.023689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.023606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.023945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.023708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.023573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.023639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.023713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.023771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.023765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.023699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.023744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.023724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.023650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.023695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.023752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.023595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.023776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.023904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.023830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.023825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.023904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.023828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.023803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.023928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.023848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.023843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.023857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.023786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.023735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.023719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.023893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.024059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.023913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.024007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.024031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.023991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.024029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.023972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.023956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.023953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.024019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.023943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.023940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.023989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.023979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.023983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.023999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.024032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.024054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.024031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.024051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.024054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.024060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.024055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.024055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.024064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.024057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.024059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Training finished!\n",
      "‚úÖ Model saved to ./continued_training_results\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# Function to load and continue training\n",
    "# ================================================================================\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def continue_training(model_path, train_file, test_file, output_dir, num_train_epochs=3):\n",
    "    \"\"\"\n",
    "    Loads an existing model and continues training.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the existing model.\n",
    "        train_file (str): Path to the training data file.\n",
    "        test_file (str): Path to the testing data file.\n",
    "        output_dir (str): Directory to save the continued training results.\n",
    "        num_train_epochs (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    # Load the datasets\n",
    "    raw_datasets = load_dataset('json', data_files={'train': train_file, 'test': test_file})\n",
    "\n",
    "    # Define the prefix for the task\n",
    "    prefix = \"\"\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenizes the questions and answers, ensuring answers are strings.\"\"\"\n",
    "        inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # Ensure every item in the \"answer\" list is a string\n",
    "        answers = [str(ans) if ans is not None else \"\" for ans in examples[\"answer\"]]\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(answers, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply the preprocessing to our entire dataset\n",
    "    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    # Create the Trainer object\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Start the training\n",
    "    print(\"üöÄ Continuing training...\")\n",
    "    trainer.train()\n",
    "    print(\"üéâ Training finished!\")\n",
    "\n",
    "    # Save the final model\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "continue_training(\n",
    "    model_path=\"./final_date_calculator_model\",\n",
    "    train_file=\"date_train.jsonl\",\n",
    "    test_file=\"date_test.jsonl\",\n",
    "    output_dir=\"./continued_training_results\",\n",
    "    num_train_epochs=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442bcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'date_calculator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m question = item[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     21\u001b[39m ground_truth = item[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result = date_calculator(prefix + question)\n\u001b[32m     23\u001b[39m predicted_answer = result[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     25\u001b[39m predictions.append(predicted_answer)\n",
      "\u001b[31mNameError\u001b[39m: name 'date_calculator' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the test dataset\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "test_data = load_test_data('date_test.jsonl')\n",
    "\n",
    "# Run inference on the test dataset\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "# Open the output file\n",
    "with open('output.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for item in tqdm(test_data):\n",
    "        question = item['question']\n",
    "        ground_truth = item['answer']\n",
    "        result = date_calculator(prefix + question)\n",
    "        predicted_answer = result[0]['generated_text']\n",
    "\n",
    "        predictions.append(predicted_answer)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        # Write the question, answer, and predicted answer to the output file\n",
    "        output_file.write(f\"Question: {question}\\n\")\n",
    "        output_file.write(f\"Answer: {ground_truth}\\n\")\n",
    "        output_file.write(f\"Predicted: {predicted_answer}\\n\")\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy = accuracy_score(ground_truths, predictions)\n",
    "precision = precision_score(ground_truths, predictions, average='weighted')\n",
    "recall = recall_score(ground_truths, predictions, average='weighted')\n",
    "f1 = f1_score(ground_truths, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
